{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ea686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       condition  condition_order  attention_check\n",
      "0        predict            215.0              1.0\n",
      "1        predict             68.0              1.0\n",
      "2        predict             12.0              1.0\n",
      "3        predict            214.0              1.0\n",
      "4        predict            197.0              1.0\n",
      "..           ...              ...              ...\n",
      "413  accommodate            148.0              1.0\n",
      "414  accommodate            188.0              1.0\n",
      "415  accommodate             22.0              1.0\n",
      "416  accommodate            178.0              1.0\n",
      "417  accommodate            125.0              1.0\n",
      "\n",
      "[418 rows x 3 columns]\n",
      "\n",
      "PREDICT – failed attention check:\n",
      "0\n",
      "None\n",
      "\n",
      "ACCOMMODATE – failed attention check:\n",
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import csv\n",
    "\n",
    "topdir = '/Users/sm6511/Desktop/Prediction-Accomodation-Exp'\n",
    "study = 'Study2.0'\n",
    "studyn = 220\n",
    "dates = [\n",
    "    '2026-02-07',\n",
    "    '2026-02-09'\n",
    "]\n",
    "datadir = os.path.join(topdir, f'data/{study}/Predict')\n",
    "datadir2 = os.path.join(topdir, f'data/{study}/Accommodate')\n",
    "\n",
    "\n",
    "#Determine whether attention check was correct\n",
    "def extract_basic_info(csv_path, condition):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if condition == 'accommodate':\n",
    "        att_col = 'button_3_correct.numClicks'\n",
    "    if condition == 'predict':\n",
    "        att_col = 'answer_3_right.numClicks'\n",
    "    if att_col in df.columns:\n",
    "        #print(f\"Found attention check column in {os.path.basename(csv_path)}\")\n",
    "        att_rows = df[df[att_col].notna()]\n",
    "        if not att_rows.empty:\n",
    "            att_val = att_rows.iloc[0][att_col]\n",
    "        else:\n",
    "            att_val = np.nan\n",
    "    else:\n",
    "        att_val = np.nan  # column never existed for this participant\n",
    "    if att_col not in df.columns:\n",
    "        print(f\"Missing attention check column in {os.path.basename(csv_path)}\") #If the column doesn't exist, warn me as the data must be incomplete\n",
    "\n",
    "\n",
    "    participant_id = df['participant_id'].dropna().iloc[0]\n",
    "\n",
    "    return {\n",
    "        'condition': condition,\n",
    "        'condition_order': participant_id,\n",
    "        'attention_check': att_val\n",
    "    }\n",
    "\n",
    "\n",
    "all_participants = []\n",
    "#Repeat for predict/accommodate\n",
    "for fname in os.listdir(datadir):\n",
    "    if fname.endswith('.csv') and fname:\n",
    "        participant_id = fname[:3]\n",
    "        if not any(d in fname for d in dates):\n",
    "            continue\n",
    "        csv_path = os.path.join(datadir, fname)\n",
    "        #print(csv_path)\n",
    "        info = extract_basic_info(csv_path, condition='predict')\n",
    "        all_participants.append(info)\n",
    "\n",
    "for fname in os.listdir(datadir2):\n",
    "    if fname.endswith('.csv') and fname:\n",
    "        participant_id = fname[:3]\n",
    "        if not any(d in fname for d in dates):\n",
    "            continue\n",
    "        csv_path = os.path.join(datadir2, fname)\n",
    "        #print(csv_path)\n",
    "        info = extract_basic_info(csv_path, condition='accommodate')\n",
    "        all_participants.append(info)\n",
    "\n",
    "\n",
    "#Combine and look at who failed the attention check\n",
    "df_all = pd.DataFrame(all_participants)\n",
    "print(df_all)\n",
    "df_all['attention_check'] = pd.to_numeric(\n",
    "    df_all['attention_check'],\n",
    "    errors='coerce'\n",
    ")\n",
    "failed = df_all[df_all['attention_check'] == 0]\n",
    "\n",
    "for cond in ['predict', 'accommodate']:\n",
    "    failed_ids = failed.loc[\n",
    "        failed['condition'] == cond,\n",
    "        'condition_order'\n",
    "    ].tolist()\n",
    "\n",
    "    print(f\"\\n{cond.upper()} – failed attention check:\")\n",
    "    print(len(failed_ids))\n",
    "    print(failed_ids if failed_ids else \"None\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73aa265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICT failed IDs: ['091', '149', '033', '208', '191', '016', '017', '080', '156', '152', '023', '025', '056', '113', '104', '061', '186', '049', '171', '207', '188', '118', '147', '094', '182', '214', '192', '149', '161', '087', '053', '065', '148', '162', '066', '096', '019', '082', '146', '030']\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/091_test_2026-02-07_10h54.32.507.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/149_test_2026-02-07_12h18.15.146.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/033_test_2026-02-07_10h47.10.696.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/208_test_2026-02-07_10h07.42.991.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/191_test_2026-02-07_11h06.43.750.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/016_test_2026-02-07_09h48.24.049.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/017_test_2026-02-07_09h46.20.037.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/080_test_2026-02-07_10h52.23.931.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/156_test_2026-02-07_09h01.45.646.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/152_test_2026-02-07_11h04.23.451.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/023_test_2026-02-07_10h46.37.819.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/025_test_2026-02-07_07h47.15.109.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/056_test_2026-02-07_11h14.22.139.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/113_test_2026-02-07_09h56.28.920.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/104_test_2026-02-07_10h54.41.364.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/061_test_2026-02-07_08h49.54.973.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/186_test_2026-02-07_11h24.26.312.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/049_test_2026-02-07_10h48.08.002.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/171_test_2026-02-07_11h03.39.827.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/207_test_2026-02-07_11h10.11.841.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/188_test_2026-02-07_08h05.33.716.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/118_test_2026-02-07_11h18.55.874.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/147_test_2026-02-07_11h00.12.240.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/094_test_2026-02-07_10h53.38.059.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/182_test_2026-02-07_11h03.46.652.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/214_test_2026-02-07_08h08.46.187.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/192_test_2026-02-07_10h11.45.478.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/149_test_2026-02-07_11h25.22.471.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/161_test_2026-02-07_08h01.53.990.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/087_test_2026-02-07_10h53.01.646.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/053_test_2026-02-07_09h48.57.964.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/065_test_2026-02-07_10h49.52.267.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/148_test_2026-02-07_11h00.39.658.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/162_test_2026-02-07_10h22.39.964.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/066_test_2026-02-07_10h46.49.639.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/096_test_2026-02-07_10h07.51.400.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/019_test_2026-02-07_10h47.03.168.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/082_test_2026-02-07_10h51.37.908.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/146_test_2026-02-07_11h00.39.078.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Predict/030_test_2026-02-07_09h47.18.270.csv\n",
      "PREDICT – deleted files:\n",
      "40\n",
      "['091_test_2026-02-07_10h54.32.507.csv', '149_test_2026-02-07_12h18.15.146.csv', '033_test_2026-02-07_10h47.10.696.csv', '208_test_2026-02-07_10h07.42.991.csv', '191_test_2026-02-07_11h06.43.750.csv', '016_test_2026-02-07_09h48.24.049.csv', '017_test_2026-02-07_09h46.20.037.csv', '080_test_2026-02-07_10h52.23.931.csv', '156_test_2026-02-07_09h01.45.646.csv', '152_test_2026-02-07_11h04.23.451.csv', '023_test_2026-02-07_10h46.37.819.csv', '025_test_2026-02-07_07h47.15.109.csv', '056_test_2026-02-07_11h14.22.139.csv', '113_test_2026-02-07_09h56.28.920.csv', '104_test_2026-02-07_10h54.41.364.csv', '061_test_2026-02-07_08h49.54.973.csv', '186_test_2026-02-07_11h24.26.312.csv', '049_test_2026-02-07_10h48.08.002.csv', '171_test_2026-02-07_11h03.39.827.csv', '207_test_2026-02-07_11h10.11.841.csv', '188_test_2026-02-07_08h05.33.716.csv', '118_test_2026-02-07_11h18.55.874.csv', '147_test_2026-02-07_11h00.12.240.csv', '094_test_2026-02-07_10h53.38.059.csv', '182_test_2026-02-07_11h03.46.652.csv', '214_test_2026-02-07_08h08.46.187.csv', '192_test_2026-02-07_10h11.45.478.csv', '149_test_2026-02-07_11h25.22.471.csv', '161_test_2026-02-07_08h01.53.990.csv', '087_test_2026-02-07_10h53.01.646.csv', '053_test_2026-02-07_09h48.57.964.csv', '065_test_2026-02-07_10h49.52.267.csv', '148_test_2026-02-07_11h00.39.658.csv', '162_test_2026-02-07_10h22.39.964.csv', '066_test_2026-02-07_10h46.49.639.csv', '096_test_2026-02-07_10h07.51.400.csv', '019_test_2026-02-07_10h47.03.168.csv', '082_test_2026-02-07_10h51.37.908.csv', '146_test_2026-02-07_11h00.39.078.csv', '030_test_2026-02-07_09h47.18.270.csv']\n",
      "\n",
      "ACCOMMODATE failed IDs: ['132', '026', '100', '196', '114', '185', '010', '181', '129', '191', '192', '025', '216', '039', '206', '045', '091', '051', '011', '094', '013', '042', '049', '123', '002', '117', '071', '161', '072', '210', '113', '220', '165', '082', '213', '187', '186', '012', '212']\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/132_explain2_2026-02-07_07h59.16.806.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/026_explain2_2026-02-07_10h46.56.420.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/100_explain2_2026-02-07_07h54.22.499.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/196_explain2_2026-02-07_11h27.05.951.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/114_explain2_2026-02-07_10h58.10.971.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/185_explain2_2026-02-07_11h03.57.662.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/010_explain2_2026-02-07_11h13.17.492.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/181_explain2_2026-02-07_07h09.03.172.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/129_explain2_2026-02-07_10h01.08.310.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/191_explain2_2026-02-07_08h55.46.177.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/192_explain2_2026-02-07_10h26.12.098.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/025_explain2_2026-02-07_10h46.43.290.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/216_explain2_2026-02-07_11h10.52.325.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/039_explain2_2026-02-07_09h30.26.574.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/206_explain2_2026-02-07_11h07.13.493.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/045_explain2_2026-02-07_07h48.12.935.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/091_explain2_2026-02-07_10h53.14.624.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/051_explain2_2026-02-07_10h48.50.818.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/011_explain2_2026-02-07_11h11.15.061.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/094_explain2_2026-02-07_09h54.13.530.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/013_explain2_2026-02-07_09h46.11.080.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/042_explain2_2026-02-07_08h49.03.299.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/049_explain2_2026-02-07_09h48.17.251.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/123_explain2_2026-02-07_08h51.21.147.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/002_explain2_2026-02-07_10h02.56.645.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/117_explain2_2026-02-07_11h20.05.389.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/071_explain2_2026-02-07_10h50.41.394.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/161_explain2_2026-02-07_11h23.03.270.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/072_explain2_2026-02-07_11h23.36.149.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/210_explain2_2026-02-07_11h08.00.188.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/113_explain2_2026-02-07_11h25.12.659.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/220_explain2_2026-02-07_11h12.36.967.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/165_explain2_2026-02-07_10h03.58.823.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/082_explain2_2026-02-07_10h52.31.269.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/213_explain2_2026-02-07_12h09.10.460.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/187_explain2_2026-02-07_10h05.32.861.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/186_explain2_2026-02-07_10h04.19.220.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/012_explain2_2026-02-07_07h46.34.750.csv\n",
      "DELETING: /Users/sm6511/Desktop/Prediction-Accomodation-Exp/data/Study2.0/Accommodate/212_explain2_2026-02-07_11h08.18.582.csv\n",
      "ACCOMMODATE – deleted files:\n",
      "39\n",
      "['132_explain2_2026-02-07_07h59.16.806.csv', '026_explain2_2026-02-07_10h46.56.420.csv', '100_explain2_2026-02-07_07h54.22.499.csv', '196_explain2_2026-02-07_11h27.05.951.csv', '114_explain2_2026-02-07_10h58.10.971.csv', '185_explain2_2026-02-07_11h03.57.662.csv', '010_explain2_2026-02-07_11h13.17.492.csv', '181_explain2_2026-02-07_07h09.03.172.csv', '129_explain2_2026-02-07_10h01.08.310.csv', '191_explain2_2026-02-07_08h55.46.177.csv', '192_explain2_2026-02-07_10h26.12.098.csv', '025_explain2_2026-02-07_10h46.43.290.csv', '216_explain2_2026-02-07_11h10.52.325.csv', '039_explain2_2026-02-07_09h30.26.574.csv', '206_explain2_2026-02-07_11h07.13.493.csv', '045_explain2_2026-02-07_07h48.12.935.csv', '091_explain2_2026-02-07_10h53.14.624.csv', '051_explain2_2026-02-07_10h48.50.818.csv', '011_explain2_2026-02-07_11h11.15.061.csv', '094_explain2_2026-02-07_09h54.13.530.csv', '013_explain2_2026-02-07_09h46.11.080.csv', '042_explain2_2026-02-07_08h49.03.299.csv', '049_explain2_2026-02-07_09h48.17.251.csv', '123_explain2_2026-02-07_08h51.21.147.csv', '002_explain2_2026-02-07_10h02.56.645.csv', '117_explain2_2026-02-07_11h20.05.389.csv', '071_explain2_2026-02-07_10h50.41.394.csv', '161_explain2_2026-02-07_11h23.03.270.csv', '072_explain2_2026-02-07_11h23.36.149.csv', '210_explain2_2026-02-07_11h08.00.188.csv', '113_explain2_2026-02-07_11h25.12.659.csv', '220_explain2_2026-02-07_11h12.36.967.csv', '165_explain2_2026-02-07_10h03.58.823.csv', '082_explain2_2026-02-07_10h52.31.269.csv', '213_explain2_2026-02-07_12h09.10.460.csv', '187_explain2_2026-02-07_10h05.32.861.csv', '186_explain2_2026-02-07_10h04.19.220.csv', '012_explain2_2026-02-07_07h46.34.750.csv', '212_explain2_2026-02-07_11h08.18.582.csv']\n"
     ]
    }
   ],
   "source": [
    "def delete_failed_files(failed_ids, datadir, dates):\n",
    "    deleted = []\n",
    "\n",
    "    for fname in os.listdir(datadir):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        if not any(d in fname for d in dates):\n",
    "            continue\n",
    "\n",
    "        participant_id = fname[:3].strip()\n",
    "\n",
    "        if participant_id in failed_ids:\n",
    "            path = os.path.join(datadir, fname)\n",
    "            print(\"DELETING:\", path)\n",
    "            os.remove(path)\n",
    "            deleted.append(fname)\n",
    "\n",
    "    return deleted\n",
    "\n",
    "\n",
    "for cond, directory in [\n",
    "    ('predict', datadir),\n",
    "    ('accommodate', datadir2)\n",
    "]:\n",
    "    failed_ids = (\n",
    "        failed.loc[failed['condition'] == cond, 'condition_order']\n",
    "        .astype(int)\n",
    "        .astype(str)\n",
    "        .str.zfill(3)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{cond.upper()} failed IDs:\", failed_ids)\n",
    "\n",
    "    deleted_files = delete_failed_files(failed_ids, directory, dates)\n",
    "\n",
    "    print(f\"{cond.upper()} – deleted files:\")\n",
    "    print(len(deleted_files))\n",
    "    print(deleted_files if deleted_files else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bed45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING CONDITION ORDERS (001–220):\n",
      "['002', '010', '013', '023', '049', '087', '094', '113', '182', '186', '210']\n",
      "\n",
      "MISSING CONDITION ORDERS (within each condition):\n",
      "PREDICT: ['002', '010', '013', '023', '049', '087', '094', '113', '182', '186', '210']\n",
      "ACCOMMODATE: ['002', '010', '013', '023', '049', '087', '094', '113', '182', '186', '210']\n",
      "\n",
      "DUPLICATE CONDITION ORDERS (within each condition):\n",
      "PREDICT: None\n",
      "ACCOMMODATE: None\n"
     ]
    }
   ],
   "source": [
    "'''Check for duplicate or missing entries'''\n",
    "\n",
    "observed_ids = (\n",
    "    df_all['condition_order']\n",
    "    .dropna()\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "expected_ids = set(range(1, studyn + 1))\n",
    "observed_ids_set = set(observed_ids)\n",
    "missing_ids = sorted(expected_ids - observed_ids_set)\n",
    "\n",
    "# Print as 3-digit IDs\n",
    "missing_ids_str = [f\"{i:03d}\" for i in missing_ids]\n",
    "\n",
    "print(\"\\nMISSING CONDITION ORDERS (001–220):\")\n",
    "print(missing_ids_str if missing_ids_str else \"None \")\n",
    "\n",
    "expected_ids = set(range(1, studyn + 1))\n",
    "\n",
    "print(\"\\nMISSING CONDITION ORDERS (within each condition):\")\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_orders = (\n",
    "        df_all[df_all['condition'] == cond]['condition_order']\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    missing = sorted(expected_ids - set(cond_orders))\n",
    "    missing_str = [f\"{i:03d}\" for i in missing]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"{cond.upper()}: {missing_str}\")\n",
    "    else:\n",
    "        print(f\"{cond.upper()}: None\")\n",
    "\n",
    "#crete dataframe of missing rows\n",
    "\n",
    "missing_rows = []\n",
    "\n",
    "expected_ids = set(range(1, studyn + 1))\n",
    "\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_orders = (\n",
    "        df_all[df_all['condition'] == cond]['condition_order']\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    missing = expected_ids - set(cond_orders)\n",
    "\n",
    "    for mid in missing:\n",
    "        missing_rows.append({\n",
    "            'condition': cond,\n",
    "            'condition_order': mid,\n",
    "            'attention_check': np.nan  # not applicable\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_rows)\n",
    "# Check duplicates **within each condition**\n",
    "print(\"\\nDUPLICATE CONDITION ORDERS (within each condition):\")\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_duplicates = (\n",
    "        df_all[df_all['condition'] == cond]\n",
    "        .duplicated(subset='condition_order', keep=False)\n",
    "    )\n",
    "    dup_df = df_all[(df_all['condition'] == cond) & cond_duplicates]\n",
    "    \n",
    "    if dup_df.empty:\n",
    "        print(f\"{cond.upper()}: None\")\n",
    "    else:\n",
    "        print(f\"{cond.upper()}:\")\n",
    "        print(dup_df[['condition_order', 'condition']].sort_values('condition_order'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a79b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 82 URLs to /Users/sm6511/Desktop/Prediction-Accomodation-Exp/ConditionFiles-Prolific/failed_participants_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def generate_failed_participants_csv_by_condition(\n",
    "    failed_df,\n",
    "    output_file=\"failed_participants_by_condition.csv\",\n",
    "    url_map=None,\n",
    "    pad=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV with Pavlovia links for failed attention check participants,\n",
    "    one link per participant per condition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    failed_df : pd.DataFrame\n",
    "        Must have columns: ['condition_order', 'condition']\n",
    "    output_file : str\n",
    "        Name of the CSV file to save\n",
    "    url_map : dict\n",
    "        Mapping from condition name to URL prefix\n",
    "        {'predict': 'https://run.pavlovia.org/montesinos7/test?condition=',\n",
    "               'accommodate': 'https://run.pavlovia.org/montesinos7/explain2?condition='}\n",
    "    pad : int\n",
    "        Number of digits to pad participant IDs (e.g., 3 -> '001')\n",
    "    \"\"\"\n",
    "    if url_map is None:\n",
    "        url_map = {\n",
    "            'predict': 'https://run.pavlovia.org/montesinos7/predict2?condition=',\n",
    "            'accommodate': 'https://run.pavlovia.org/montesinos7/accommodate2?condition='\n",
    "        }\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for _, row in failed_df.iterrows():\n",
    "            pid_padded = str(int(float(row['condition_order']))).zfill(pad)\n",
    "            url = url_map.get(row['condition'], None)\n",
    "            if url:\n",
    "                writer.writerow([url + pid_padded])\n",
    "\n",
    "    print(f\"Saved {len(failed_df)} URLs to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "#Combine missing runs and failed runs\n",
    "failed_plus_missing = pd.concat(\n",
    "    [failed[['condition', 'condition_order']], \n",
    "     missing_df[['condition', 'condition_order']]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "# Failed attention check DataFrame:\n",
    "failed_plus_missing['condition_order'] = failed_plus_missing['condition_order'].astype(float)  # just in case\n",
    "generate_failed_participants_csv_by_condition(\n",
    "    failed_plus_missing,\n",
    "    output_file=\"/Users/sm6511/Desktop/Prediction-Accomodation-Exp/ConditionFiles-Prolific/failed_participants_combined.csv\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredictProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
