{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3ea686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       condition  condition_order  attention_check\n",
      "0        predict            100.0              1.0\n",
      "1        predict             60.0              1.0\n",
      "2        predict             33.0              1.0\n",
      "3        predict             20.0              1.0\n",
      "4        predict             43.0              1.0\n",
      "..           ...              ...              ...\n",
      "295  accommodate             75.0              1.0\n",
      "296  accommodate             54.0              1.0\n",
      "297  accommodate            139.0              1.0\n",
      "298  accommodate             16.0              1.0\n",
      "299  accommodate             69.0              1.0\n",
      "\n",
      "[300 rows x 3 columns]\n",
      "\n",
      "PREDICT – failed attention check:\n",
      "None\n",
      "\n",
      "ACCOMMODATE – failed attention check:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import csv\n",
    "\n",
    "topdir = '/Users/sm6511/Desktop/Prediction-Accomodation-Exp'\n",
    "study = 'Study1.0'\n",
    "dates = [\n",
    "    '2026-01-12',\n",
    "    '2026-01-13',\n",
    "    '2026-01-14',\n",
    "]\n",
    "datadir = os.path.join(topdir, f'data/{study}/Predict')\n",
    "datadir2 = os.path.join(topdir, f'data/{study}/Accommodate')\n",
    "\n",
    "\n",
    "#Determine whether attention check was correct\n",
    "def extract_basic_info(csv_path, condition):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if condition == 'accommodate':\n",
    "        att_col = 'button_3_correct.numClicks'\n",
    "    if condition == 'predict':\n",
    "        att_col = 'answer_3_right.numClicks'\n",
    "    if att_col in df.columns:\n",
    "        #print(f\"Found attention check column in {os.path.basename(csv_path)}\")\n",
    "        att_rows = df[df[att_col].notna()]\n",
    "        if not att_rows.empty:\n",
    "            att_val = att_rows.iloc[0][att_col]\n",
    "        else:\n",
    "            att_val = np.nan\n",
    "    else:\n",
    "        att_val = np.nan  # column never existed for this participant\n",
    "    if att_col not in df.columns:\n",
    "        print(f\"Missing attention check column in {os.path.basename(csv_path)}\") #If the column doesn't exist, warn me as the data must be incomplete\n",
    "\n",
    "\n",
    "    participant_id = df['participant_id'].dropna().iloc[0]\n",
    "\n",
    "    return {\n",
    "        'condition': condition,\n",
    "        'condition_order': participant_id,\n",
    "        'attention_check': att_val\n",
    "    }\n",
    "\n",
    "\n",
    "all_participants = []\n",
    "#Repeat for predict/accommodate\n",
    "for fname in os.listdir(datadir):\n",
    "    if fname.endswith('.csv') and fname:\n",
    "        participant_id = fname[:3]\n",
    "        if not any(d in fname for d in dates):\n",
    "            continue\n",
    "        csv_path = os.path.join(datadir, fname)\n",
    "        #print(csv_path)\n",
    "        info = extract_basic_info(csv_path, condition='predict')\n",
    "        all_participants.append(info)\n",
    "\n",
    "for fname in os.listdir(datadir2):\n",
    "    if fname.endswith('.csv') and fname:\n",
    "        participant_id = fname[:3]\n",
    "        if not any(d in fname for d in dates):\n",
    "            continue\n",
    "        csv_path = os.path.join(datadir2, fname)\n",
    "        #print(csv_path)\n",
    "        info = extract_basic_info(csv_path, condition='accommodate')\n",
    "        all_participants.append(info)\n",
    "\n",
    "\n",
    "#Combine and look at who failed the attention check\n",
    "df_all = pd.DataFrame(all_participants)\n",
    "print(df_all)\n",
    "df_all['attention_check'] = pd.to_numeric(\n",
    "    df_all['attention_check'],\n",
    "    errors='coerce'\n",
    ")\n",
    "failed = df_all[df_all['attention_check'] == 0]\n",
    "\n",
    "for cond in ['predict', 'accommodate']:\n",
    "    failed_ids = failed.loc[\n",
    "        failed['condition'] == cond,\n",
    "        'condition_order'\n",
    "    ].tolist()\n",
    "\n",
    "    print(f\"\\n{cond.upper()} – failed attention check:\")\n",
    "    print(failed_ids if failed_ids else \"None\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bed45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING CONDITION ORDERS (001–150):\n",
      "None \n",
      "\n",
      "MISSING CONDITION ORDERS (within each condition):\n",
      "PREDICT: None\n",
      "ACCOMMODATE: None\n",
      "\n",
      "DUPLICATE CONDITION ORDERS (within each condition):\n",
      "PREDICT: None\n",
      "ACCOMMODATE: None\n"
     ]
    }
   ],
   "source": [
    "'''Check for duplicate or missing entries'''\n",
    "\n",
    "observed_ids = (\n",
    "    df_all['condition_order']\n",
    "    .dropna()\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "expected_ids = set(range(1, 151))\n",
    "observed_ids_set = set(observed_ids)\n",
    "missing_ids = sorted(expected_ids - observed_ids_set)\n",
    "\n",
    "# Print as 3-digit IDs\n",
    "missing_ids_str = [f\"{i:03d}\" for i in missing_ids]\n",
    "\n",
    "print(\"\\nMISSING CONDITION ORDERS (001–150):\")\n",
    "print(missing_ids_str if missing_ids_str else \"None \")\n",
    "\n",
    "expected_ids = set(range(1, 151))\n",
    "\n",
    "print(\"\\nMISSING CONDITION ORDERS (within each condition):\")\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_orders = (\n",
    "        df_all[df_all['condition'] == cond]['condition_order']\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    missing = sorted(expected_ids - set(cond_orders))\n",
    "    missing_str = [f\"{i:03d}\" for i in missing]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"{cond.upper()}: {missing_str}\")\n",
    "    else:\n",
    "        print(f\"{cond.upper()}: None\")\n",
    "\n",
    "#crete dataframe of missing rows\n",
    "\n",
    "missing_rows = []\n",
    "\n",
    "expected_ids = set(range(1, 151))\n",
    "\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_orders = (\n",
    "        df_all[df_all['condition'] == cond]['condition_order']\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    missing = expected_ids - set(cond_orders)\n",
    "\n",
    "    for mid in missing:\n",
    "        missing_rows.append({\n",
    "            'condition': cond,\n",
    "            'condition_order': mid,\n",
    "            'attention_check': np.nan  # not applicable\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_rows)\n",
    "# Check duplicates **within each condition**\n",
    "print(\"\\nDUPLICATE CONDITION ORDERS (within each condition):\")\n",
    "for cond in df_all['condition'].unique():\n",
    "    cond_duplicates = (\n",
    "        df_all[df_all['condition'] == cond]\n",
    "        .duplicated(subset='condition_order', keep=False)\n",
    "    )\n",
    "    dup_df = df_all[(df_all['condition'] == cond) & cond_duplicates]\n",
    "    \n",
    "    if dup_df.empty:\n",
    "        print(f\"{cond.upper()}: None\")\n",
    "    else:\n",
    "        print(f\"{cond.upper()}:\")\n",
    "        print(dup_df[['condition_order', 'condition']].sort_values('condition_order'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a79b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2 URLs to /Users/sm6511/Desktop/Prediction-Accomodation-Exp/ConditionFiles-Prolific/failed_participants_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def generate_failed_participants_csv_by_condition(\n",
    "    failed_df,\n",
    "    output_file=\"failed_participants_by_condition.csv\",\n",
    "    url_map=None,\n",
    "    pad=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a CSV with Pavlovia links for failed attention check participants,\n",
    "    one link per participant per condition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    failed_df : pd.DataFrame\n",
    "        Must have columns: ['condition_order', 'condition']\n",
    "    output_file : str\n",
    "        Name of the CSV file to save\n",
    "    url_map : dict\n",
    "        Mapping from condition name to URL prefix\n",
    "        {'predict': 'https://run.pavlovia.org/montesinos7/test?condition=',\n",
    "               'accommodate': 'https://run.pavlovia.org/montesinos7/explain2?condition='}\n",
    "    pad : int\n",
    "        Number of digits to pad participant IDs (e.g., 3 -> '001')\n",
    "    \"\"\"\n",
    "    if url_map is None:\n",
    "        url_map = {\n",
    "            'predict': 'https://run.pavlovia.org/montesinos7/test?condition=',\n",
    "            'accommodate': 'https://run.pavlovia.org/montesinos7/explain2?condition='\n",
    "        }\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for _, row in failed_df.iterrows():\n",
    "            pid_padded = str(int(float(row['condition_order']))).zfill(pad)\n",
    "            url = url_map.get(row['condition'], None)\n",
    "            if url:\n",
    "                writer.writerow([url + pid_padded])\n",
    "\n",
    "    print(f\"Saved {len(failed_df)} URLs to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "#Combine missing runs and failed runs\n",
    "failed_plus_missing = pd.concat(\n",
    "    [failed[['condition', 'condition_order']], \n",
    "     missing_df[['condition', 'condition_order']]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "# Failed attention check DataFrame:\n",
    "failed_plus_missing['condition_order'] = failed_plus_missing['condition_order'].astype(float)  # just in case\n",
    "generate_failed_participants_csv_by_condition(\n",
    "    failed_plus_missing,\n",
    "    output_file=\"/Users/sm6511/Desktop/Prediction-Accomodation-Exp/ConditionFiles-Prolific/failed_participants_combined.csv\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredictProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
